{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     register(\n",
    "#             id='FrozenLakeNoSlip-v0',\n",
    "#             entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#             kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "#             max_episode_steps=100,\n",
    "#             reward_threshold=0.78, # optimum = .8196\n",
    "#             )\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     register(\n",
    "#             id='KellyCoinfliptest-v0',\n",
    "#             entry_point='gym.envs.toy_text:KellyCoinflipEnv',\n",
    "#             kwargs={'initial_wealth':25.0, 'edge':0.6, 'max_wealth':250.0, 'max_rounds' : 10.0},\n",
    "#             reward_threshold=246.61,\n",
    "# )\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v1'\n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "#env_name = 'Acrobot-v1'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLakeNoSlip-v0'\n",
    "#env_name = 'Taxi-v3'\n",
    "#env_name = 'Gamble-v0'\n",
    "#env_name = 'KellyCoinflip-v0'\n",
    "env_name = 'Cliff_edited-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#goal_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  72\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 0.6                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "#         for i in range(38,46):\n",
    "#             self.q_table[i][0] = 0\n",
    "#             self.q_table[i][1] = 0.3\n",
    "#             self.q_table[i][2] = -0.3\n",
    "#             self.q_table[i][3] = 0\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "        \n",
    "    def get_action_greedy(self, state):\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "        elif state == next_state:\n",
    "             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(72)\n",
      "72 4\n",
      "0.6\n",
      "\n",
      "WWWWWWWWWWWW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "W\u001b[41mS\u001b[0mCCCCCCCCGW\n",
      "WWWWWWWWWWWW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nUP = 0\\nRIGHT = 1\\nDOWN = 2\\nLEFT = 3\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "env.render()\n",
    "'''\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 2)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#        print(\"state: \", state, \"action: \", action)\n",
    "#        print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.eps = 0\n",
    "#agent.eps = 0.1\n",
    "np.savetxt('q_table.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.6\\nrandom 1, decays by 0.99하면 400번에서 어느 정도 학습은 된다 (최적 경로 X, 답을 찾긴하지만 오래 걸린다)\\n                             500번에서 거의 최적 경로로 학습한다.\\n                             \\nState 38-45이 아래로 이동하면 cliff으로 떨어지는 구간. Q-table score은 대략 -31 ~ -8 까지\\n해당 구간에 1이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾음\\n해당 구간에 5이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾\\n\\n1.7\\n랜덤 initial을 더 낮게, decay를 0.999로\\n최적해를 처음 찾는 training session의 number(?)\\ntraining test 번갈아가면서 (with & without random)\\n\\n1.13\\n0.996 decay with initial random 0.6 ==> 800번 반복했을 때 random이 0.0243\\n더 빠르게 학습 가능할 수 있도록 lower random initial, lower decay\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "12.3\n",
    "2개정도 줄이기. (faster simulation)\n",
    "보상과 step penalty 차이가 너무 크다.\n",
    "\n",
    "12.9\n",
    "cliff penalty를 -10으로 줄였더니 계속 첫 자리에 남아있으려는 경향이 있다 (벽쪽으로 이동하려는 경향)\n",
    "==> 사각에 Cliff 추가 후 학습 잘 됨 (optimal route). Goal reward 50, Cliff penalty -10, step penalty -1\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "12.23\n",
    "W(벽) 만들어서 실험 해보니 성공적으로 제자리 걸음 안하게 했다.\n",
    "현재 1000번 학습하면 최단거리로 길 잘 찾는다.\n",
    "\n",
    "안전한 경로에 대한 사전정보 & 잘못된 사전정보에 대한 결과 확인해보기\n",
    "'''\n",
    "\n",
    "'''\n",
    "1.6\n",
    "random 1, decays by 0.99하면 400번에서 어느 정도 학습은 된다 (최적 경로 X, 답을 찾긴하지만 오래 걸린다)\n",
    "                             500번에서 거의 최적 경로로 학습한다.\n",
    "                             \n",
    "State 38-45이 아래로 이동하면 cliff으로 떨어지는 구간. Q-table score은 대략 -31 ~ -8 까지\n",
    "해당 구간에 1이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾음\n",
    "해당 구간에 5이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾\n",
    "\n",
    "1.7\n",
    "랜덤 initial을 더 낮게, decay를 0.999로\n",
    "최적해를 처음 찾는 training session의 number(?)\n",
    "training test 번갈아가면서 (with & without random)\n",
    "\n",
    "1.13\n",
    "0.996 decay with initial random 0.6 ==> 800번 반복했을 때 random이 0.0243\n",
    "더 빠르게 학습 가능할 수 있도록 lower random initial, lower decay\n",
    "\n",
    "1.19\n",
    "0.99 decay with initial random 0.6\n",
    "0.5 쯤부터는 학습을 아예 실패하는 경우가 생긴다\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     count = 0\n",
    "#     while not done:\n",
    "#         action = agent.get_action_greedy(state)                    # decide on an action\n",
    "#         next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "#         if state==next_state or count>15:\n",
    "#             break\n",
    "#         else:\n",
    "#             state = next_state\n",
    "#             env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#             time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "#             clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "# # states 38-45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#        print(\"state: \", state, \"action: \", action)\n",
    "#        print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal2.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table2.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table2.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#        print(\"state: \", state, \"action: \", action)\n",
    "#        print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal3.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table3.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table3.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal4.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table4.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table4.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal5.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table5.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table5.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal6.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table6.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table6.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal7.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table7.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table7.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal8.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table8.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table8.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal9.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table9.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table9.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal10.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table10.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table10.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#         print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal11.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table11.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table11.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal12.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table12.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table12.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal13.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table13.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table13.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#    print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#        print(\"state: \", state, \"action: \", action)\n",
    "#        print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal14.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table14.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table14.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal15.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table15.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table15.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal16.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table16.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table16.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal17.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table17.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table17.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record > 0:\n",
    "        with open('first_optimal18.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table18.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table18.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal19.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table19.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table19.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(env)\n",
    "total_reward = 0\n",
    "for i in range(3000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "#        env.render()                       # show the screen of the game\n",
    "#         print_softmax_array(agent.q_table)\n",
    "#        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "    state = env.reset()\n",
    "    reward_record = 0\n",
    "    done = False\n",
    "    count = 0\n",
    "#     print(state)\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        reward_record = reward\n",
    "        if state==next_state or count>11:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "    if reward_record == 30:\n",
    "        with open('first_optimal20.txt', 'w') as f:\n",
    "            f.write('%d' % i)\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "np.savetxt('q_table20.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table20.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.4269458773438117\n"
     ]
    }
   ],
   "source": [
    "# # agent = QAgent(env)\n",
    "# for i in range(10):\n",
    "#     state = env.reset()\n",
    "#     reward_record = 0\n",
    "#     done = False\n",
    "#     count = 0\n",
    "#     print(state)\n",
    "#     while not done:\n",
    "#         action = agent.get_action_greedy(state)                    # decide on an action\n",
    "#         next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         count = count+1\n",
    "# #         print(\"state: \", state, \"action: \", action)\n",
    "# #         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "#         reward_record = reward\n",
    "#         if state==next_state or count>11:\n",
    "#             break\n",
    "#         else:\n",
    "#             state = next_state\n",
    "#             env.render()                       # show the screen of the game\n",
    "# #             print(agent.q_table)               # show q-table after every action\n",
    "#             time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "#             clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    \n",
    "#     if reward_record == 30:\n",
    "#         with open('first_optimal20.txt', 'w') as f:\n",
    "#             f.write('%d' % i)\n",
    "#         print(i)\n",
    "#         break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "print(agent.q_table[37][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
