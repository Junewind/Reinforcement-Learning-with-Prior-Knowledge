{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     register(\n",
    "#             id='FrozenLakeNoSlip-v0',\n",
    "#             entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#             kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "#             max_episode_steps=100,\n",
    "#             reward_threshold=0.78, # optimum = .8196\n",
    "#             )\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     register(\n",
    "#             id='KellyCoinfliptest-v0',\n",
    "#             entry_point='gym.envs.toy_text:KellyCoinflipEnv',\n",
    "#             kwargs={'initial_wealth':25.0, 'edge':0.6, 'max_wealth':250.0, 'max_rounds' : 10.0},\n",
    "#             reward_threshold=246.61,\n",
    "# )\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v1'\n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "#env_name = 'Acrobot-v1'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLakeNoSlip-v0'\n",
    "#env_name = 'Taxi-v3'\n",
    "#env_name = 'Gamble-v0'\n",
    "#env_name = 'KellyCoinflip-v0'\n",
    "env_name = 'Cliff_edited-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#goal_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  72\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "        for i in range(39,47):\n",
    "            self.q_table[i][2] = 5\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "        elif state == next_state:\n",
    "             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(72)\n",
      "72 4\n",
      "1.0\n",
      "\n",
      "WWWWWWWWWWWW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "W\u001b[41mS\u001b[0mCCCCCCCCGW\n",
      "WWWWWWWWWWWW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nUP = 0\\nRIGHT = 1\\nDOWN = 2\\nLEFT = 3\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "env.render()\n",
    "'''\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.007 0.007 0.98  0.007]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 2)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  46 action:  2\n",
      "Training Session:  4     Total reward:  400     randomness:  0.0\n",
      "  (Down)\n",
      "WWWWWWWWWWWW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "WPPPPPPPPPPW\n",
      "WSCCCCCCCC\u001b[41mG\u001b[0mW\n",
      "WWWWWWWWWWWW\n",
      "[[0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.271 0.272 0.271 0.186]\n",
      " [0.205 0.265 0.264 0.266]\n",
      " [0.239 0.255 0.253 0.254]\n",
      " [0.222 0.26  0.261 0.258]\n",
      " [0.232 0.257 0.256 0.255]\n",
      " [0.2   0.267 0.267 0.266]\n",
      " [0.225 0.258 0.259 0.258]\n",
      " [0.219 0.261 0.262 0.258]\n",
      " [0.249 0.249 0.252 0.25 ]\n",
      " [0.211 0.233 0.284 0.273]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.271 0.269 0.27  0.19 ]\n",
      " [0.251 0.25  0.249 0.249]\n",
      " [0.25  0.25  0.25  0.251]\n",
      " [0.251 0.251 0.25  0.248]\n",
      " [0.238 0.24  0.284 0.239]\n",
      " [0.212 0.212 0.367 0.209]\n",
      " [0.188 0.189 0.435 0.188]\n",
      " [0.136 0.139 0.587 0.137]\n",
      " [0.157 0.158 0.529 0.157]\n",
      " [0.008 0.006 0.978 0.008]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.279 0.28  0.28  0.161]\n",
      " [0.335 0.334 0.    0.331]\n",
      " [0.332 0.335 0.    0.333]\n",
      " [0.26  0.48  0.    0.26 ]\n",
      " [0.017 0.965 0.    0.017]\n",
      " [0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.   ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.349 0.    0.336 0.315]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.29956854e+00 -2.29709820e+00 -2.30204233e+00 -2.67696630e+00]\n",
      " [-2.37657286e+00 -2.12048829e+00 -2.12434365e+00 -2.11473722e+00]\n",
      " [-1.90272132e+00 -1.83958314e+00 -1.84586706e+00 -1.84306408e+00]\n",
      " [-1.65486239e+00 -1.49766992e+00 -1.49450461e+00 -1.50639763e+00]\n",
      " [-1.22478977e+00 -1.12453548e+00 -1.12609731e+00 -1.13262767e+00]\n",
      " [-1.04661746e+00 -7.57547166e-01 -7.59484739e-01 -7.63028838e-01]\n",
      " [-5.85198506e-01 -4.46443197e-01 -4.42888322e-01 -4.48086492e-01]\n",
      " [-3.94039900e-01 -2.16630313e-01 -2.14386658e-01 -2.26845130e-01]\n",
      " [-1.00000000e-01 -9.76237956e-02 -8.87729863e-02 -9.33451906e-02]\n",
      " [-2.97010000e-01 -1.99000000e-01  1.46844765e-04 -4.02781720e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.46827092e+00 -2.47289131e+00 -2.47124173e+00 -2.82269467e+00]\n",
      " [-2.20340818e+00 -2.20583599e+00 -2.21104455e+00 -2.21053629e+00]\n",
      " [-1.86009136e+00 -1.86092940e+00 -1.85882442e+00 -1.85684804e+00]\n",
      " [-1.44905849e+00 -1.44985105e+00 -1.45048941e+00 -1.46016102e+00]\n",
      " [-1.01745534e+00 -1.01107360e+00 -8.43158342e-01 -1.01531625e+00]\n",
      " [-6.12791823e-01 -6.08836804e-01 -6.30323315e-02 -6.24004943e-01]\n",
      " [-2.99593472e-01 -2.93988201e-01  5.39063936e-01 -2.99406993e-01]\n",
      " [-1.22679910e-01 -1.02478633e-01  1.33740595e+00 -1.14442795e-01]\n",
      " [-3.02798725e-02 -2.44649184e-02  1.18742787e+00 -3.07250201e-02]\n",
      " [-1.99970000e-02 -1.99000000e-01  4.84148782e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.82819474e+00 -2.82293158e+00 -2.82373759e+00 -3.37717959e+00]\n",
      " [-2.33588656e+00 -2.33962215e+00 -4.18833586e+01 -2.34802014e+00]\n",
      " [-1.86055322e+00 -1.85166265e+00 -1.58287881e+01 -1.85688417e+00]\n",
      " [-1.33687088e+00 -7.22062501e-01 -1.83287573e+01 -1.33499681e+00]\n",
      " [-8.68917405e-01  3.14674873e+00 -1.83287573e+01 -8.71150144e-01]\n",
      " [-4.78430193e-01  1.22525087e+01 -9.69387276e+00 -4.82267706e-01]\n",
      " [-1.74529152e-01  2.85682514e+01 -4.08068901e+00 -1.56925620e-01]\n",
      " [-5.92856521e-02  5.16523943e+01 -3.11180709e+00 -9.13124006e-03]\n",
      " [-1.86906560e-02  7.63534257e+01 -1.14458431e+00  7.02306782e-02]\n",
      " [ 4.09447975e-02 -4.90099501e-01  9.49510575e+01  5.30006042e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-3.40494336e+00 -3.88882760e+01 -3.44340779e+00 -3.50897372e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        print_softmax_array(agent.q_table)\n",
    "        print(agent.q_table)               # show q-table after every action\n",
    "        time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eps = 0\n",
    "#agent.eps = 0.1\n",
    "np.savetxt('q_table.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.6\\nrandom 1, decays by 0.99하면 400번에서 어느 정도 학습은 된다 (최적 경로 X, 답을 찾긴하지만 오래 걸린다)\\n                             500번에서 거의 최적 경로로 학습한다.\\n                             \\nState 39-46이 아래로 이동하면 cliff으로 떨어지는 구간. Q-table score은 대략 -31 ~ -8 까지\\n해당 구간에 1이라는 잘못된 prior를 줬을때는 학습속도에 영향 X\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "12.3\n",
    "2개정도 줄이기. (faster simulation)\n",
    "보상과 step penalty 차이가 너무 크다.\n",
    "\n",
    "12.9\n",
    "cliff penalty를 -10으로 줄였더니 계속 첫 자리에 남아있으려는 경향이 있다 (벽쪽으로 이동하려는 경향)\n",
    "==> 사각에 Cliff 추가 후 학습 잘 됨 (optimal route). Goal reward 50, Cliff penalty -10, step penalty -1\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "12.23\n",
    "W(벽) 만들어서 실험 해보니 성공적으로 제자리 걸음 안하게 했다.\n",
    "현재 1000번 학습하면 최단거리로 길 잘 찾는다.\n",
    "\n",
    "안전한 경로에 대한 사전정보 & 잘못된 사전정보에 대한 결과 확인해보기\n",
    "'''\n",
    "\n",
    "'''\n",
    "1.6\n",
    "random 1, decays by 0.99하면 400번에서 어느 정도 학습은 된다 (최적 경로 X, 답을 찾긴하지만 오래 걸린다)\n",
    "                             500번에서 거의 최적 경로로 학습한다.\n",
    "                             \n",
    "State 39-46이 아래로 이동하면 cliff으로 떨어지는 구간. Q-table score은 대략 -31 ~ -8 까지\n",
    "해당 구간에 1이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾음\n",
    "해당 구간에 5이라는 잘못된 prior를 줬을때 500번 학습하면 성공적으로 길 찾\n",
    "\n",
    "1.7\n",
    "랜덤 initial을 더 낮게, decay를 0.999로\n",
    "최적해를 처음 찾는 training session의 number(?)\n",
    "training test 번갈아가면서 (with & without random)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 연구에 써볼만한 환경 연구해보기 (3개)\n",
    "# cliff-walking\n",
    "# kelly coinflip\n",
    "\n",
    "# pacman\n",
    "\n",
    "# maze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
