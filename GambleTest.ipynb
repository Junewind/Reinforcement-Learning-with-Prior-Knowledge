{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Gamble-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(31)\n",
      "31 4\n",
      "1.0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Current wealth:  20.0 ; Rounds left:  10\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "print(agent.q_table)\n",
    "env.render()\n",
    "\n",
    "# q-table 더 잘 보기 위한 확률 기반 q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 3)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  9 action:  0\n",
      "Training Session:  299     Total reward:  11064     randomness:  0.7414484806367364\n",
      "Current wealth:  52 ; Rounds left:  0\n",
      "[[0.248 0.231 0.276 0.245]\n",
      " [0.153 0.219 0.217 0.411]\n",
      " [0.162 0.32  0.151 0.367]\n",
      " [0.298 0.183 0.213 0.306]\n",
      " [0.201 0.213 0.192 0.395]\n",
      " [0.514 0.112 0.245 0.13 ]\n",
      " [0.448 0.127 0.21  0.215]\n",
      " [0.088 0.085 0.713 0.114]\n",
      " [0.06  0.134 0.11  0.696]\n",
      " [0.315 0.298 0.282 0.105]\n",
      " [0.753 0.086 0.048 0.113]\n",
      " [0.268 0.156 0.493 0.083]\n",
      " [0.031 0.073 0.074 0.822]\n",
      " [0.356 0.281 0.157 0.206]\n",
      " [0.083 0.035 0.046 0.836]\n",
      " [0.153 0.693 0.079 0.075]\n",
      " [0.167 0.071 0.692 0.07 ]\n",
      " [0.114 0.613 0.117 0.156]\n",
      " [0.865 0.045 0.044 0.046]\n",
      " [0.011 0.026 0.012 0.951]\n",
      " [0.135 0.137 0.132 0.596]\n",
      " [0.265 0.282 0.085 0.369]\n",
      " [0.014 0.013 0.059 0.915]\n",
      " [0.006 0.006 0.978 0.01 ]\n",
      " [0.021 0.022 0.86  0.096]\n",
      " [0.249 0.249 0.249 0.252]\n",
      " [0.063 0.812 0.063 0.063]\n",
      " [0.134 0.134 0.598 0.134]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.476 0.476 0.024 0.024]\n",
      " [0.25  0.25  0.25  0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.eps = 1\n",
    "agent.eps = 0\n",
    "#print(total_reward)\n",
    "#print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n12.23\\n사전 정보 없이 1000번 학습 후 10회 결과 (randomness = 1.0부터 *0.999로 decay):\\n1회차: 204\\n2회차: 583\\n3회차: 104\\n4회차: 336\\n5회차: 484\\n==> 평균: 342.2   표준편차: 175.48150899739\\n\\n시도해볼 사전 정보는 0.01. 최종적으로는 0.1 내외의 값이 q-table에 저장되므로 10%정도의 사전정보라고 판단했다.\\n\\n사전 정보 가지고 1000번 학습:\\n- 사전 정보: action 0(0.1 베팅) = 0.01\\n1회차: 456\\n2회차: 383\\n3회차: 321\\n4회차: 557\\n5회차: 621\\n==> 평균: 467.6    표준편차: 109.82458741102\\n\\n사전 정보 가지고 1000번 학습:\\n- 사전 정보: action 1(0.2 베팅) = 0.01\\n1회차: 437\\n2회차: 325\\n3회차: 261\\n4회차: 346\\n5회차: 538\\n==> 평균: 381.4    표준편차: 96.483366442097\\n\\n- 사전 정보: action 3(0.6 베팅) = 0.01\\n1회차: 220\\n2회차: 426\\n3회차: 265\\n4회차: 318\\n5회차: 154\\n==> 평균: 276.6    표준편차: 92.09039037815\\n\\n결과와 상관없이 안정적으로 선택하게 된다.\\n\\nq-table을 block처럼\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "12.1\n",
    "계속 0이 가장 안전하니까 성공하기에도 가장 좋다고 판단하는듯.\n",
    "\n",
    "12.9\n",
    "0에 대한 penalty를 만들었더니 도박 하려는 경향 증가. 그러나 적은 액수에서는 도박 해도 어차피 얻는게 없어서 돈 없을 때는 도박을 안한다.\n",
    "initial wealth, success rate 상향조정\n",
    "\n",
    "\n",
    "초기 자금 20, success rate 0.65로 하고 action을 [0, 20%, 40%, 60%] 도박으로 하면 40 혹은 60을 자주 선택한다.\n",
    "평균 초기 자금의 90% 내외를 벌고 있다. ==> 최대치에 도달 했을 때 done인 것을 취소하거나 최대치를 높여야할 것 같다.\n",
    "==> 최대치 200으로 상향조정 ==> 돈을 오히려 더 많이 잘 번다. ==> success rate 다시 0.6으로 조정\n",
    "==> 다시 0, 20% 위주로 한다. ==> 0.62로 조정 ==> 일정 수량 이상으로 가면 돈 아끼려는 경향이 보임 ==> hold penalty를 0.9에서 0.85로 조정\n",
    "==> hold을 없애교 그냥 0.1 도박으로 조정 ==> 최종 action 목록:[10%, 20%, 40%, 60%]\n",
    "\n",
    "이제는 골고루 이것저것 시도해본다.\n",
    "'''\n",
    "\n",
    "'''\n",
    "12.23\n",
    "사전 정보 없이 1000번 학습 후 10회 결과 (randomness = 1.0부터 *0.999로 decay):\n",
    "1회차: 204\n",
    "2회차: 583\n",
    "3회차: 104\n",
    "4회차: 336\n",
    "5회차: 484\n",
    "==> 평균: 342.2   표준편차: 175.48150899739\n",
    "\n",
    "시도해볼 사전 정보는 0.01. 최종적으로는 0.1 내외의 값이 q-table에 저장되므로 10%정도의 사전정보라고 판단했다.\n",
    "\n",
    "사전 정보 가지고 1000번 학습:\n",
    "- 사전 정보: action 0(0.1 베팅) = 0.01\n",
    "1회차: 456\n",
    "2회차: 383\n",
    "3회차: 321\n",
    "4회차: 557\n",
    "5회차: 621\n",
    "==> 평균: 467.6    표준편차: 109.82458741102\n",
    "\n",
    "사전 정보 가지고 1000번 학습:\n",
    "- 사전 정보: action 1(0.2 베팅) = 0.01\n",
    "1회차: 437\n",
    "2회차: 325\n",
    "3회차: 261\n",
    "4회차: 346\n",
    "5회차: 538\n",
    "==> 평균: 381.4    표준편차: 96.483366442097\n",
    "\n",
    "- 사전 정보: action 3(0.6 베팅) = 0.01\n",
    "1회차: 220\n",
    "2회차: 426\n",
    "3회차: 265\n",
    "4회차: 318\n",
    "5회차: 154\n",
    "==> 평균: 276.6    표준편차: 92.09039037815\n",
    "\n",
    "결과와 상관없이 안정적으로 선택하게 된다.\n",
    "\n",
    "q-table을 block처럼\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('q_table.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = env.wealth\n",
    "    total_reward += env.wealth\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table2.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table2.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list2.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list2.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table3.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table3.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list3.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list3.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table4.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table4.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list4.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list4.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table5.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table5.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list5.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list5.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "435\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table6.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table6.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list6.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list6.csv', money_array, delimiter=',')\n",
    "print(total_reward)\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table7.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table7.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list7.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list7.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table8.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table8.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list8.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list8.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table9.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table9.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list9.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list9.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table10.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table10.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list10.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list10.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table11.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table11.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list11.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list11.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table12.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table12.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list12.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list12.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table13.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table13.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list13.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list13.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table14.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table14.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list14.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list14.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table15.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table15.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list15.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list15.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "581\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table16.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table16.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list16.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list16.csv', money_array, delimiter=',')\n",
    "\n",
    "print(total_reward)\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table17.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table17.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list17.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list17.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table18.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table18.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list18.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list18.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table19.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table19.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list19.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list19.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table20.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table20.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list20.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list20.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
