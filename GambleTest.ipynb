{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Gamble-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # how much to update the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(31)\n",
      "31 4\n",
      "1.0\n",
      "[[0.12657989 0.13478144 0.21337874 0.29090808]\n",
      " [0.54817722 0.77803416 0.79439719 0.85600043]\n",
      " [0.94489635 0.99238679 1.02770998 1.10070199]\n",
      " [1.21941676 1.10495167 1.28898284 1.48511871]\n",
      " [1.31568412 1.3624673  1.50075357 1.37553902]\n",
      " [1.49589546 1.42914645 1.62756057 1.52041209]\n",
      " [1.22906602 1.43003036 1.2433691  1.2287811 ]\n",
      " [1.35249361 1.33641781 1.41266268 1.5805303 ]\n",
      " [1.36241645 1.03797825 1.56131066 1.28167796]\n",
      " [1.03875234 1.01435687 1.11172365 1.0327792 ]\n",
      " [1.19915407 1.27999196 0.83413312 1.07300731]\n",
      " [1.15783573 0.89811577 0.78006141 0.94476969]\n",
      " [1.16741435 0.68727139 1.05514946 1.44676088]\n",
      " [1.3489256  0.82944368 0.69065583 0.95038071]\n",
      " [0.92097354 0.85301083 1.37922451 1.20919247]\n",
      " [0.87330564 0.96045611 0.83639617 0.47028804]\n",
      " [0.69803015 0.61165529 0.97770922 0.87132625]\n",
      " [0.62726285 0.75964514 0.92030874 0.46943472]\n",
      " [0.80863647 0.5114523  0.20285433 0.79543065]\n",
      " [0.6085705  0.8934974  0.35776391 4.18705218]\n",
      " [0.34996775 0.48060523 0.54728494 2.31850454]\n",
      " [0.4859952  0.30575498 1.20395241 2.39340537]\n",
      " [0.29829221 0.36507318 1.26368797 2.23129795]\n",
      " [0.47777082 0.43453723 1.99628025 1.41398178]\n",
      " [0.62574506 0.42468318 2.15250635 1.53772792]\n",
      " [0.56579557 0.97384157 1.00735013 0.97458673]\n",
      " [0.0805891  0.98042698 1.0580537  0.84498371]\n",
      " [0.80837932 0.72749322 0.66984429 0.72926432]\n",
      " [1.37519682 0.52569941 0.15069292 0.59936343]\n",
      " [1.67984653 0.49806948 0.37890565 0.97377687]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Current wealth:  20.0 ; Rounds left:  10\n"
     ]
    }
   ],
   "source": [
    "agent.q_table = np.genfromtxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\no prior.csv', delimiter=',')\n",
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "print(agent.q_table)\n",
    "env.render()\n",
    "\n",
    "# q-table 더 잘 보기 위한 확률 기반 q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.234 0.236 0.255 0.276]\n",
      " [0.204 0.257 0.261 0.278]\n",
      " [0.232 0.244 0.252 0.272]\n",
      " [0.234 0.209 0.251 0.306]\n",
      " [0.232 0.243 0.279 0.246]\n",
      " [0.244 0.228 0.278 0.25 ]\n",
      " [0.236 0.289 0.239 0.236]\n",
      " [0.232 0.229 0.247 0.292]\n",
      " [0.259 0.187 0.316 0.239]\n",
      " [0.247 0.241 0.266 0.246]\n",
      " [0.273 0.296 0.19  0.241]\n",
      " [0.306 0.236 0.21  0.248]\n",
      " [0.261 0.161 0.233 0.345]\n",
      " [0.359 0.214 0.186 0.241]\n",
      " [0.206 0.193 0.326 0.275]\n",
      " [0.269 0.293 0.259 0.179]\n",
      " [0.226 0.207 0.299 0.268]\n",
      " [0.231 0.263 0.309 0.197]\n",
      " [0.305 0.227 0.167 0.301]\n",
      " [0.026 0.034 0.02  0.92 ]\n",
      " [0.095 0.108 0.116 0.681]\n",
      " [0.094 0.079 0.193 0.634]\n",
      " [0.086 0.092 0.226 0.595]\n",
      " [0.11  0.106 0.503 0.281]\n",
      " [0.112 0.092 0.517 0.279]\n",
      " [0.18  0.27  0.279 0.27 ]\n",
      " [0.121 0.298 0.322 0.26 ]\n",
      " [0.269 0.248 0.234 0.249]\n",
      " [0.458 0.196 0.135 0.211]\n",
      " [0.482 0.148 0.131 0.238]\n",
      " [0.25  0.25  0.25  0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 3)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for testnum in range(20):\n",
    "    total_reward = 0\n",
    "    for i in range(300):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "        #    action = env.action_space.sample() # choosing a random action\n",
    "            action = agent.get_action(state)                    # decide on an action\n",
    "            next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "    #         print(\"state: \", state, \"action: \", action)\n",
    "    #         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "            agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "            state = next_state\n",
    "    #         env.render()                       # show the screen of the game\n",
    "            #print(agent.q_table)               # show q-table after every action\n",
    "    #         print_softmax_array(agent.q_table)\n",
    "            #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "        agent.eps = agent.eps * 0.999\n",
    "        total_reward += reward\n",
    "\n",
    "    env.close()\n",
    "    np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\q_table%d.csv' % testnum, agent.q_table, delimiter=',')\n",
    "    np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\softmax_q_table%d.csv' % testnum, softmax_array(agent.q_table), delimiter=',')\n",
    "    \n",
    "    agent.eps = 0\n",
    "    total_reward = 0\n",
    "    action_array = np.zeros([10, 10])\n",
    "    money_array =  np.zeros([10, 11])\n",
    "    \n",
    "    for i in range(10):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        j = 0\n",
    "        while not done:\n",
    "        #    action = env.action_space.sample() # choosing a random action\n",
    "            money_array[i][j] = env.wealth\n",
    "            action = agent.get_action(state)                    # decide on an action\n",
    "            action_array[i][j] = action\n",
    "            j = j+1\n",
    "            next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#             print(\"state: \", state, \"action: \", action)\n",
    "#             print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "            agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "            state = next_state\n",
    "#             env.render()                       # show the screen of the game\n",
    "            #print(agent.q_table)               # show q-table after every action\n",
    "#             print_softmax_array(agent.q_table)\n",
    "            #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "        money_array[i][10] = env.wealth\n",
    "        total_reward += env.wealth\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(total_reward)\n",
    "    np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\action_list%d.csv' % testnum, action_array, delimiter=',')\n",
    "    np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\money_list%d.csv' % testnum, money_array, delimiter=',')\n",
    "\n",
    "    class QAgent(Agent):\n",
    "        def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "            super().__init__(env)\n",
    "            self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "            print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "\n",
    "            self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "            self.discount_rate = discount_rate\n",
    "            self.learning_rate = learning_rate\n",
    "            self.build_model()\n",
    "\n",
    "        def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "            #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "            self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "\n",
    "\n",
    "        def get_action(self, state):\n",
    "            '''\n",
    "            Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "            Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "            '''\n",
    "            q_state = self.q_table[state]                 # current state\n",
    "            action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "            action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "            if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "                return action_random\n",
    "            else:\n",
    "                return action_greedy\n",
    "\n",
    "        def train(self, experience):\n",
    "            state, action, next_state, reward, done = experience\n",
    "\n",
    "            q_next = self.q_table[next_state]\n",
    "            # current state is terminal\n",
    "            if done:\n",
    "                q_next = np.zeros([self.action_size])\n",
    "    #         elif state == next_state:\n",
    "    #             q_next = np.zeros([self.action_size])\n",
    "            else:\n",
    "                q_next\n",
    "\n",
    "            # what the next action is based on the q-table\n",
    "            q_target = reward + self.discount_rate*np.max(q_next)\n",
    "\n",
    "            # update table\n",
    "            q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "            self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "\n",
    "            # reduce randomness after each epoch\n",
    "\n",
    "            # penalty 함수도 있어야한다.\n",
    "            # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "    agent = QAgent(env)\n",
    "    agent.q_table = np.genfromtxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\no prior.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.eps = 1\n",
    "agent.eps = 0\n",
    "#print(total_reward)\n",
    "#print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n12.23\\n사전 정보 없이 1000번 학습 후 10회 결과 (randomness = 1.0부터 *0.999로 decay):\\n1회차: 204\\n2회차: 583\\n3회차: 104\\n4회차: 336\\n5회차: 484\\n==> 평균: 342.2   표준편차: 175.48150899739\\n\\n시도해볼 사전 정보는 0.01. 최종적으로는 0.1 내외의 값이 q-table에 저장되므로 10%정도의 사전정보라고 판단했다.\\n\\n사전 정보 가지고 1000번 학습:\\n- 사전 정보: action 0(0.1 베팅) = 0.01\\n1회차: 456\\n2회차: 383\\n3회차: 321\\n4회차: 557\\n5회차: 621\\n==> 평균: 467.6    표준편차: 109.82458741102\\n\\n사전 정보 가지고 1000번 학습:\\n- 사전 정보: action 1(0.2 베팅) = 0.01\\n1회차: 437\\n2회차: 325\\n3회차: 261\\n4회차: 346\\n5회차: 538\\n==> 평균: 381.4    표준편차: 96.483366442097\\n\\n- 사전 정보: action 3(0.6 베팅) = 0.01\\n1회차: 220\\n2회차: 426\\n3회차: 265\\n4회차: 318\\n5회차: 154\\n==> 평균: 276.6    표준편차: 92.09039037815\\n\\n결과와 상관없이 안정적으로 선택하게 된다.\\n\\nq-table을 block처럼\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "12.1\n",
    "계속 0이 가장 안전하니까 성공하기에도 가장 좋다고 판단하는듯.\n",
    "\n",
    "12.9\n",
    "0에 대한 penalty를 만들었더니 도박 하려는 경향 증가. 그러나 적은 액수에서는 도박 해도 어차피 얻는게 없어서 돈 없을 때는 도박을 안한다.\n",
    "initial wealth, success rate 상향조정\n",
    "\n",
    "\n",
    "초기 자금 20, success rate 0.65로 하고 action을 [0, 20%, 40%, 60%] 도박으로 하면 40 혹은 60을 자주 선택한다.\n",
    "평균 초기 자금의 90% 내외를 벌고 있다. ==> 최대치에 도달 했을 때 done인 것을 취소하거나 최대치를 높여야할 것 같다.\n",
    "==> 최대치 200으로 상향조정 ==> 돈을 오히려 더 많이 잘 번다. ==> success rate 다시 0.6으로 조정\n",
    "==> 다시 0, 20% 위주로 한다. ==> 0.62로 조정 ==> 일정 수량 이상으로 가면 돈 아끼려는 경향이 보임 ==> hold penalty를 0.9에서 0.85로 조정\n",
    "==> hold을 없애교 그냥 0.1 도박으로 조정 ==> 최종 action 목록:[10%, 20%, 40%, 60%]\n",
    "\n",
    "이제는 골고루 이것저것 시도해본다.\n",
    "'''\n",
    "\n",
    "'''\n",
    "12.23\n",
    "사전 정보 없이 1000번 학습 후 10회 결과 (randomness = 1.0부터 *0.999로 decay):\n",
    "1회차: 204\n",
    "2회차: 583\n",
    "3회차: 104\n",
    "4회차: 336\n",
    "5회차: 484\n",
    "==> 평균: 342.2   표준편차: 175.48150899739\n",
    "\n",
    "시도해볼 사전 정보는 0.01. 최종적으로는 0.1 내외의 값이 q-table에 저장되므로 10%정도의 사전정보라고 판단했다.\n",
    "\n",
    "사전 정보 가지고 1000번 학습:\n",
    "- 사전 정보: action 0(0.1 베팅) = 0.01\n",
    "1회차: 456\n",
    "2회차: 383\n",
    "3회차: 321\n",
    "4회차: 557\n",
    "5회차: 621\n",
    "==> 평균: 467.6    표준편차: 109.82458741102\n",
    "\n",
    "사전 정보 가지고 1000번 학습:\n",
    "- 사전 정보: action 1(0.2 베팅) = 0.01\n",
    "1회차: 437\n",
    "2회차: 325\n",
    "3회차: 261\n",
    "4회차: 346\n",
    "5회차: 538\n",
    "==> 평균: 381.4    표준편차: 96.483366442097\n",
    "\n",
    "- 사전 정보: action 3(0.6 베팅) = 0.01\n",
    "1회차: 220\n",
    "2회차: 426\n",
    "3회차: 265\n",
    "4회차: 318\n",
    "5회차: 154\n",
    "==> 평균: 276.6    표준편차: 92.09039037815\n",
    "\n",
    "결과와 상관없이 안정적으로 선택하게 된다.\n",
    "\n",
    "q-table을 block처럼\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\q_table.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\No Prior as Prior\\softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = env.wealth\n",
    "    total_reward += env.wealth\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\action_list.csv', action_array, delimiter=',')\n",
    "np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Gamble Environment\\money_list.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table2.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table2.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list2.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list2.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table3.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table3.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list3.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list3.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table4.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table4.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list4.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list4.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table5.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table5.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list5.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list5.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "435\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table6.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table6.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list6.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list6.csv', money_array, delimiter=',')\n",
    "print(total_reward)\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table7.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table7.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list7.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list7.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table8.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table8.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list8.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list8.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table9.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table9.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list9.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list9.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table10.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table10.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list10.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list10.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table11.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table11.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list11.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list11.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table12.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table12.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list12.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list12.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table13.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table13.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list13.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list13.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table14.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table14.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list14.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list14.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table15.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table15.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list15.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list15.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "581\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table16.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table16.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list16.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list16.csv', money_array, delimiter=',')\n",
    "\n",
    "print(total_reward)\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table17.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table17.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list17.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list17.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table18.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table18.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list18.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list18.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table19.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table19.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list19.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list19.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Action size:  4\n",
      "State size:  31\n"
     ]
    }
   ],
   "source": [
    "agent.eps = 1\n",
    "total_reward = 0\n",
    "for i in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "agent.eps = 0\n",
    "np.savetxt('q_table20.csv', agent.q_table, delimiter=',')\n",
    "np.savetxt('softmax_q_table20.csv', softmax_array(agent.q_table), delimiter=',')\n",
    "\n",
    "total_reward = 0\n",
    "action_array = np.zeros([10, 10])\n",
    "money_array =  np.zeros([10, 11])\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        money_array[i][j] = env.wealth\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        action_array[i][j] = action\n",
    "        j = j+1\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        #print(agent.q_table)               # show q-table after every action\n",
    "        print_softmax_array(agent.q_table)\n",
    "        #time.sleep(0.3)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    money_array[i][10] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(total_reward)\n",
    "np.savetxt('action_list20.csv', action_array, delimiter=',')\n",
    "np.savetxt('money_list20.csv', money_array, delimiter=',')\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 1.0                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "#         elif state == next_state:\n",
    "#             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
