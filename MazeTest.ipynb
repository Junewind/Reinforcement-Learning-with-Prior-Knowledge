{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output\n",
    "from six import StringIO, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v1'\n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "#env_name = 'Acrobot-v1'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLakeNoSlip-v0'\n",
    "#env_name = 'Taxi-v3'\n",
    "#env_name = 'Gamble-v0'\n",
    "#env_name = 'KellyCoinflip-v0'\n",
    "env_name = 'Maze_edited-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#goal_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  121\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 0.1                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "        for i in range(env.nrow):\n",
    "            for j in range(env.ncol):\n",
    "                if env.desc[i,j] in b'W':\n",
    "                    for k in range(self.action_size):\n",
    "                        self.q_table[i*env.nrow+j][k] = np.NINF\n",
    "\n",
    "        \n",
    "# #         second-best path\n",
    "#         self.q_table[16][0] = -5\n",
    "#         self.q_table[16][1] = -5\n",
    "# #         self.q_table[16][2] = 4\n",
    "#         self.q_table[16][3] = -5\n",
    "        \n",
    "\n",
    "#         self.q_table[83][0] = -5\n",
    "#         self.q_table[83][1] = -5\n",
    "# #         self.q_table[83][2] = 200\n",
    "#         self.q_table[83][3] = -5\n",
    "    \n",
    "#         self.q_table[84][0] = -5\n",
    "#         self.q_table[84][1] = -5\n",
    "# #         self.q_table[83][2] = 200\n",
    "#         self.q_table[84][3] = -5\n",
    "    \n",
    "#         self.q_table[85][0] = -5\n",
    "#         self.q_table[85][1] = -5\n",
    "#         self.q_table[85][2] = -5\n",
    "        \n",
    "#         self.q_table[94][0] = -5\n",
    "#         self.q_table[94][1] = -5\n",
    "#         self.q_table[94][2] = -5\n",
    "        \n",
    "#         self.q_table[103][0] = -5\n",
    "#         self.q_table[103][1] = -5\n",
    "# #         self.q_table[103][2] = 200\n",
    "#         self.q_table[103][3] = -5\n",
    "    \n",
    "#         self.q_table[104][0] = -5\n",
    "#         self.q_table[104][1] = -5\n",
    "#         self.q_table[104][3] = -5\n",
    "        \n",
    "#         self.q_table[105][0] = -5\n",
    "#         self.q_table[105][1] = -5\n",
    "#         self.q_table[105][2] = -5\n",
    "# #         self.q_table[105][3] = 200\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        third-best path\n",
    "        \n",
    "        self.q_table[16][0] = -5\n",
    "        self.q_table[16][1] = -5\n",
    "#         self.q_table[16][2] = 4\n",
    "        self.q_table[16][3] = -5\n",
    "    \n",
    "        self.q_table[56][0] = -5\n",
    "        self.q_table[56][1] = -5\n",
    "        self.q_table[56][3] = -5\n",
    "#         self.q_table[56][3] = 10\n",
    "\n",
    "        self.q_table[57][0] = -5\n",
    "        self.q_table[57][1] = -5\n",
    "        self.q_table[57][3] = -5\n",
    "#         self.q_table[56][3] = 10\n",
    "\n",
    "        self.q_table[58][0] = -5\n",
    "        self.q_table[58][1] = -5\n",
    "        self.q_table[58][3] = -5\n",
    "#         self.q_table[56][3] = 10\n",
    "\n",
    "        self.q_table[59][0] = -5\n",
    "        self.q_table[59][1] = -5\n",
    "        self.q_table[59][3] = -5\n",
    "#         self.q_table[56][3] = 10\n",
    "\n",
    "        self.q_table[60][0] = -5\n",
    "        self.q_table[60][1] = -5\n",
    "        self.q_table[60][3] = -5\n",
    "#         self.q_table[56][3] = 10\n",
    "\n",
    "\n",
    "        self.q_table[67][0] = -5\n",
    "        self.q_table[67][1] = -5\n",
    "        self.q_table[67][2] = -5\n",
    "#         self.q_table[67][3] = 10\n",
    "        \n",
    "        self.q_table[78][0] = -5\n",
    "        self.q_table[78][1] = -5\n",
    "        self.q_table[78][2] = -5\n",
    "#         self.q_table[78][3] = 10\n",
    "        \n",
    "#         self.q_table[79][0] = 10\n",
    "        self.q_table[79][1] = -5\n",
    "        self.q_table[79][2] = -5\n",
    "        self.q_table[79][3] = -5\n",
    "        \n",
    "#         self.q_table[80][0] = 10\n",
    "        self.q_table[80][1] = -5\n",
    "        self.q_table[80][2] = -5\n",
    "        self.q_table[80][3] = -5\n",
    "        \n",
    "#         self.q_table[81][0] = 10\n",
    "        self.q_table[81][1] = -5\n",
    "        self.q_table[81][2] = -5\n",
    "        self.q_table[81][3] = -5\n",
    "        \n",
    "        self.q_table[92][0] = -5\n",
    "        self.q_table[92][1] = -5\n",
    "        self.q_table[92][2] = -5\n",
    "#         self.q_table[92][3] = 10\n",
    "        \n",
    "        \n",
    "        self.q_table[103][0] = -5\n",
    "        self.q_table[103][1] = -5\n",
    "        self.q_table[103][2] = -5\n",
    "#         self.q_table[103][2] = 10\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     optimal path\n",
    "\n",
    "#         self.q_table[16][0] = -5\n",
    "#         self.q_table[16][1] = -5\n",
    "# #         self.q_table[16][2] = 4\n",
    "#         self.q_table[16][3] = -5\n",
    "    \n",
    "# #         self.q_table[45][0] = -10\n",
    "# #         self.q_table[45][1] = -10\n",
    "# #         self.q_table[45][2] = -10\n",
    "# # #         self.q_table[56][3] = 10\n",
    "    \n",
    "#         self.q_table[56][0] = -5\n",
    "#         self.q_table[56][1] = -5\n",
    "#         self.q_table[56][2] = -5\n",
    "# #         self.q_table[56][3] = 10\n",
    "\n",
    "#         self.q_table[67][0] = -5\n",
    "#         self.q_table[67][1] = -5\n",
    "#         self.q_table[67][2] = -5\n",
    "# #         self.q_table[67][3] = 10\n",
    "        \n",
    "#         self.q_table[78][0] = -5\n",
    "#         self.q_table[78][1] = -5\n",
    "#         self.q_table[78][2] = -5\n",
    "# #         self.q_table[78][3] = 10\n",
    "        \n",
    "# #         self.q_table[79][0] = 10\n",
    "#         self.q_table[79][1] = -5\n",
    "#         self.q_table[79][2] = -5\n",
    "#         self.q_table[79][3] = -5\n",
    "        \n",
    "# #         self.q_table[80][0] = 10\n",
    "#         self.q_table[80][1] = -5\n",
    "#         self.q_table[80][2] = -5\n",
    "#         self.q_table[80][3] = -5\n",
    "        \n",
    "# #         self.q_table[81][0] = 10\n",
    "#         self.q_table[81][1] = -5\n",
    "#         self.q_table[81][2] = -5\n",
    "#         self.q_table[81][3] = -5\n",
    "        \n",
    "#         self.q_table[92][0] = -5\n",
    "#         self.q_table[92][1] = -5\n",
    "#         self.q_table[92][2] = -5\n",
    "# #         self.q_table[92][3] = 10\n",
    "        \n",
    "        \n",
    "#         self.q_table[103][0] = -5\n",
    "#         self.q_table[103][1] = -5\n",
    "#         self.q_table[103][2] = -5\n",
    "# #         self.q_table[103][2] = 10\n",
    "\n",
    "        \n",
    "\n",
    "        '''\n",
    "        LEFT = 0\n",
    "        DOWN = 1\n",
    "        RIGHT = 2\n",
    "        UP = 3\n",
    "        \n",
    "\n",
    "        '''\n",
    "\n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        \n",
    "        learning rate 조정해서 해보기\n",
    "        결과 정리\n",
    "        논문\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "        \n",
    "    def get_action_greedy(self, state):\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "        elif state == next_state:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(121)\n",
      "121 4\n",
      "0.1\n",
      "\n",
      "WWWWWWWWWWW\n",
      "WPWPPPPPPGW\n",
      "WPWPWPWWWWW\n",
      "WPPPWPPPPPW\n",
      "WPWWWWWWWPW\n",
      "WPPPPPPWPPW\n",
      "WPWWWWPWPWW\n",
      "WPPPPWPPPPW\n",
      "WWWWPWPWWWW\n",
      "W\u001b[41mS\u001b[0mPPPPPPPPW\n",
      "WWWWWWWWWWW\n",
      "[[-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]\n",
      " [-inf -inf -inf -inf]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLEFT = 0\\nDOWN = 1\\nRIGHT = 2\\nUP = 3\\n벽으로 갈 때 강한 penalty 부여\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "env.render()\n",
    "print(agent.q_table)\n",
    "'''\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "벽으로 갈 때 강한 penalty 부여\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jisoo\\AppData\\Local\\Continuum\\anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 2)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "count_record = []\n",
    "for i in range(3001):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        #env.render()                       # show the screen of the game\n",
    "#         print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    agent.eps = agent.eps * 0.99\n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "#     if i%100 == 0:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "#     np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\q_table%d.csv' % i, agent.q_table, delimiter=',')\n",
    "#     np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\softmax_q_table%d.csv' % i, softmax_array(agent.q_table), delimiter=',')\n",
    "    while not done:\n",
    "        action = agent.get_action_greedy(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        count = count+1\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Testing Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        if count>100:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "#            env.render()                       # show the screen of the game\n",
    "#             print(agent.q_table)               # show q-table after every action\n",
    "#            time.sleep(0.4)                   # 약간의 딜레이 시간 추가\n",
    "            clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "#     print(count)\n",
    "    count_record.append(count)\n",
    "\n",
    "\n",
    "np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\count_record.csv', count_record, delimiter=',')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.eps = 0\n",
    "#agent.eps = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "12.9\n",
    "reward에 대한 욕심이 없다? 그냥 계속 벽에 박는다. 앞으로 나아가는 것에 대해서 안좋게 인식.\n",
    "\n",
    "Possible solutions:\n",
    "중간에 reward\n",
    "벽에 부딪치면 stronger punishment (개선 가능성이 적어보인다)\n",
    "==>\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "벽에 부딪치는 것에 대한 punishment 증가 시킴.\n",
    "randomness 0.2로 설정해서 하니까 잘찾는다.\n",
    "100번으로는 학습이 잘 안된다.\n",
    "==> 400번 하면 학습이 어느 정도 잘 된다.\n",
    "\n",
    "더 먼 경로를 사전정보로 주면 어떤 결과가 있는지\n",
    "\n",
    "아래에 길 하나 더 뚫어보기 (best, medium, worst path)\n",
    "\n",
    "\n",
    "2000번 학습 시켜야지 최적 경로 정확하게 찾음...\n",
    "\n",
    "20.01.21\n",
    "initial 0.3, 0.99 try. 그리고 왔다갔다하는 구간 q-table 자세히 보기\n",
    "gamble에서는 no prior 평균 q-table을 사전 정보로 줘서 학습 시켜보기 (같은 학습 방식으로)\n",
    "\n",
    "20.01.12.\n",
    "state 100 is where we begin\n",
    "state 103에서 위로 가면 optimal, 우측으로 가면 2nd best (at best)\n",
    "state 56에서 위로 가면 optimal, 우측으로 가면 3rd best (at best)\n",
    "\n",
    "decision point에 대해서만 사전 정보를 주려면 엄청 크게 줘야지 학습 방향에 영향을 미친다.\n",
    "3rd optimal을 생각한 것과 다르게 움직인다?\n",
    "\n",
    "\n",
    "20.01.19\n",
    "음수를 적용한 영향으로 사이 경로가 전체적으로 lower q-value를 가지게 된다. => 상충 시키게 영향의 +-를 합친 것을 0으로 해야하나?\n",
    "+- 하니까 2개를 반복적으로 왔다갔다하는 문제가 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#     #    action = env.action_space.sample() # choosing a random action\n",
    "#         action = agent.get_action(state)                    # decide on an action\n",
    "#         next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "#         print(\"state: \", state, \"action: \", action)\n",
    "#         print(\"Training Session: \", i, \"    randomness: \", agent.eps)\n",
    "#         agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "#         state = next_state\n",
    "#         env.render()                       # show the screen of the game\n",
    "#         print(agent.q_table)               # show q-table after every action\n",
    "#         time.sleep(0.05)                   # 약간의 딜레이 시간 추가\n",
    "#         clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "#     agent.eps = agent.eps * 0.95\n",
    "# # reduce random faster\n",
    "\n",
    "# # np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\q_table.csv', agent.q_table, delimiter=',')\n",
    "# # np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(500):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#     #    action = env.action_space.sample() # choosing a random action\n",
    "#         action = agent.get_action(state)                    # decide on an action\n",
    "#         next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "# #         print(\"state: \", state, \"action: \", action)\n",
    "# #         print(\"Training Session: \", i, \"    randomness: \", agent.eps)\n",
    "#         agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "#         state = next_state\n",
    "# #         env.render()                       # show the screen of the game\n",
    "# #         print(agent.q_table)               # show q-table after every action\n",
    "# #         time.sleep(0.05)                   # 약간의 딜레이 시간 추가\n",
    "#         clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "#     agent.eps = agent.eps * 0.95\n",
    "# # reduce random faster\n",
    "\n",
    "# # np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\q_table.csv', agent.q_table, delimiter=',')\n",
    "# # np.savetxt(r'D:\\52Material\\Lab\\RL Experiment\\Maze Environment\\softmax_q_table.csv', softmax_array(agent.q_table), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
