{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register   # we use this to get rid of the slippery stuff in frozen lake\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     register(\n",
    "#             id='FrozenLakeNoSlip-v0',\n",
    "#             entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#             kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "#             max_episode_steps=100,\n",
    "#             reward_threshold=0.78, # optimum = .8196\n",
    "#             )\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     register(\n",
    "#             id='KellyCoinfliptest-v0',\n",
    "#             entry_point='gym.envs.toy_text:KellyCoinflipEnv',\n",
    "#             kwargs={'initial_wealth':25.0, 'edge':0.6, 'max_wealth':250.0, 'max_rounds' : 10.0},\n",
    "#             reward_threshold=246.61,\n",
    "# )\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v1'\n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "#env_name = 'Acrobot-v1'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLakeNoSlip-v0'\n",
    "#env_name = 'Taxi-v3'\n",
    "#env_name = 'Gamble-v0'\n",
    "#env_name = 'KellyCoinflip-v0'\n",
    "env_name = 'Maze_edited-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#goal_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = (type(env.action_space) == gym.spaces.discrete.Discrete)\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n # how many actions are available - only works for discrete\n",
    "            print(\"Action size: \", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "    \n",
    "    def get_action(self, state): # choosing an action from the available actions\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))  # discrete number of actions이기 때문에 이런 random 선정이 가능하다.\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
    "        return action\n",
    "    \n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  4\n",
      "State size:  121\n"
     ]
    }
   ],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate = 0.97, learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # 현 environment에서 가능한 discrete number of states를 값으로 받는다\n",
    "        print(\"State size: \", self.state_size)    # in this example, 16 states exist (4*4 grid)\n",
    "        \n",
    "        self.eps = 0.2                            # exploration vs. exploitation. 1.0 means 100% random\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self): # state의 갯수만큼 rows, action의 갯수만큼 column을 만들고, 랜덤하게 initializae한다.\n",
    "        #self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "        self.q_table = np.zeros([self.state_size, self.action_size])\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Just going for the \"best policy\" was bad, because it completely depends on the initialization that we have made!\n",
    "        Use randomness to search through the whole area, and as time passes we will reduce randomness\n",
    "        '''\n",
    "        q_state = self.q_table[state]                 # current state\n",
    "        action_greedy = np.argmax(q_state)            # max q-value among the possible choices\n",
    "        action_random = super().get_action(state)     # 상위 class인 agent의 get_action. 이건 random한거로 정의되어 있다\n",
    "        if random.random() < self.eps:                # random하게 0~1 값과 eps 비교\n",
    "            return action_random\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        # current state is terminal\n",
    "        if done:\n",
    "            q_next = np.zeros([self.action_size])\n",
    "        elif state == next_state:\n",
    "             q_next = np.zeros([self.action_size])\n",
    "        else:\n",
    "            q_next\n",
    "        \n",
    "        # what the next action is based on the q-table\n",
    "        q_target = reward + self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # update table\n",
    "        q_update = q_target - self.q_table[state, action]             # 해당 행동이 table과 얼마나 차이 나는지 계산 (좋은 행동이면 양수)\n",
    "        self.q_table[state, action] += self.learning_rate * q_update  # update q-table after applying learning rate\n",
    "        \n",
    "        # reduce randomness after each epoch\n",
    "        \n",
    "        # penalty 함수도 있어야한다.\n",
    "        # reward heuristic 재구성! 가까워질수록 보상이 더 있는 것이 좋다.\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(121)\n",
      "121 4\n",
      "0.2\n",
      "\n",
      "WWWWWWWWWWW\n",
      "WPWPPPPPPGW\n",
      "WPWPWPWWWWW\n",
      "WPPPWPPPPPW\n",
      "WPWWWWWWWPW\n",
      "WPPPPPPWPPW\n",
      "WPWWWWPWPWW\n",
      "WPPPPWPPPPW\n",
      "WWWWPWPWWWW\n",
      "W\u001b[41mS\u001b[0mPPPWPPPPW\n",
      "WWWWWWWWWWW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLEFT = 0\\nDOWN = 1\\nRIGHT = 2\\nUP = 3\\n벽으로 갈 때 강한 penalty 부여\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(agent.state_size, agent.action_size)\n",
    "print(agent.eps)\n",
    "env.render()\n",
    "'''\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "벽으로 갈 때 강한 penalty 부여\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "q_table_softmax = np.zeros([agent.state_size, agent.action_size])\n",
    "\n",
    "# q-table softmax화\n",
    "def softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    return np.around(q_table_softmax, decimals = 2)\n",
    "        \n",
    "def print_softmax_array(array):\n",
    "    for i in range(agent.state_size):\n",
    "        q_table_softmax[i,:]  = np.exp(array[i,:])\n",
    "        q_table_softmax[i,:] /= np.sum(q_table_softmax[i,:])\n",
    "    q_table_softmax_final = np.around(q_table_softmax, decimals = 3)\n",
    "    print(q_table_softmax_final)\n",
    "        \n",
    "print_softmax_array(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  19 action:  2\n",
      "Training Session:  499     Total reward:  49900     randomness:  0.2\n",
      "  (Right)\n",
      "WWWWWWWWWWW\n",
      "WPWPPPPPP\u001b[41mG\u001b[0mW\n",
      "WPWPWPWWWWW\n",
      "WPPPWPPPPPW\n",
      "WPWWWWWWWPW\n",
      "WPPPPPPWPPW\n",
      "WPWWWWPWPWW\n",
      "WPPPPWPPPPW\n",
      "WWWWPWPWWWW\n",
      "WSPPPWPPPPW\n",
      "WWWWWWWWWWW\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.8888276  -3.89488401 -3.8888276  -3.8888276 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.31028241 -0.65625043 29.01710086 -3.57388398]\n",
      " [ 1.247991   -2.60299627 47.71589409 -3.37717959]\n",
      " [ 3.46318809  0.67095466 67.37246564 -2.29956854]\n",
      " [ 6.81164398 -2.75019664 82.27124978 -2.82269467]\n",
      " [11.13242084 -2.96552305 92.64947221 -2.89446773]\n",
      " [18.14962212 -3.10550914 99.3429517  -2.52827906]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.94993933 -3.88075942 -3.94993933 -3.89426377]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.44340779 -1.93041753 -3.10550914 14.35889836]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.73831376 -0.68908191 -1.13615128 18.58221768]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-5.8287912  -3.95602974 -3.08369277 -3.88512343]\n",
      " [-3.28062985 -4.47316523 -0.39768921 -3.70176369]\n",
      " [-2.66413534 -3.94993933 -3.94993933  4.83684455]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.9836941  -1.65486239 -1.02492886  1.97318612]\n",
      " [-1.15242722 -2.22178641 -1.46722197 -1.9836941 ]\n",
      " [-1.89572368 -2.22178641 -1.90270765 -2.60299627]\n",
      " [-2.28246385 -2.52827906 -2.28065561 -2.60299627]\n",
      " [-2.60101194 -2.60541195 -3.94993933 -2.6769663 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-4.90014254 -4.57134217 -5.29413358 -4.44205784]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.44340779 -2.87973604 -3.37717959 -2.88379586]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-6.33967659 -5.22313055 -5.21533146 -5.21453418]\n",
      " [-5.08710167 -5.10109727 -5.07895996 -5.10109727]\n",
      " [-4.94680234 -5.0016297  -4.94255342 -5.0016297 ]\n",
      " [-4.80604035 -4.84862883 -4.7912478  -4.90014254]\n",
      " [-4.64697351 -4.79659477 -4.63719852 -4.74403512]\n",
      " [-4.48540136 -4.47738541 -5.0016297  -4.58314924]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.63814514 -3.35879779 -3.36311652 -4.07033554]\n",
      " [-3.13060826 -3.76474605 -3.37717959 -3.1326918 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-6.03322194 -5.82183725 -5.95268027 -5.812206  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-4.3609481  -4.31080461 -4.3609481  -4.31673249]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.63814514 -3.57073825 -4.18833586 -3.57477094]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-6.33967659 -6.44839185 -6.34897111 -6.33481227]\n",
      " [-6.80711789 -6.88338919 -6.81923517 -6.82010936]\n",
      " [-7.23941404 -7.23748332 -7.23979633 -7.23748332]\n",
      " [-7.63708738 -7.64237409 -7.67135537 -7.64783371]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-5.29413358 -4.14699367 -4.14635843 -4.14107647]\n",
      " [-3.96824927 -4.30398798 -3.96492902 -4.18833586]\n",
      " [-3.77621836 -4.69094457 -3.77226788 -3.77403834]\n",
      " [-3.77344513 -3.94993933 -3.82709859 -3.82709859]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-8.01725743 -8.0087669  -8.03708486 -8.01262696]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-4.2464525  -4.14736548 -4.18833586 -4.14959689]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-9.31672276 -9.31672276 -9.31517659 -9.31672276]\n",
      " [-9.01664231 -9.01886233 -9.01326881 -9.01886233]\n",
      " [-8.69866158 -8.69999655 -8.69482533 -8.69999655]\n",
      " [-8.36016562 -8.3619203  -8.3783011  -8.36233875]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-4.30398798 -4.18833586 -4.14761114 -4.14939307]\n",
      " [-4.14925325 -4.3609481  -4.14753535 -4.52843358]\n",
      " [-4.14911547 -4.30398798 -4.14823327 -4.18833586]\n",
      " [-4.14911782 -4.30398798 -4.2464525  -4.18833586]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for i in range(500):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "    #    action = env.action_space.sample() # choosing a random action\n",
    "        action = agent.get_action(state)                    # decide on an action\n",
    "        next_state, reward, done, info = env.step(action)   # doing the random action in the environment. step은 이 4가지를 return 해준다.\n",
    "        print(\"state: \", state, \"action: \", action)\n",
    "        print(\"Training Session: \", i, \"    Total reward: \", total_reward, \"    randomness: \", agent.eps)\n",
    "        agent.train((state, action, next_state, reward, done))   # 해당 action에 대해 train한다\n",
    "        state = next_state\n",
    "        env.render()                       # show the screen of the game\n",
    "        print(agent.q_table)               # show q-table after every action\n",
    "        #time.sleep(0.2)                   # 약간의 딜레이 시간 추가\n",
    "        clear_output(wait = True)         # 한 번에 1개의 action에 대한 q-table만 보이도록\n",
    "    #agent.eps = agent.eps * 0.999\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eps = 0\n",
    "#agent.eps = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n벽에 부딪치는 것에 대한 punishment 증가 시킴.\\nrandomness 0.2로 설정해서 하니까 잘찾는다.\\n100번으로는 학습이 잘 안된다.\\n==> 400번 하면 학습이 어느 정도 잘 된다.\\n\\n더 먼 경로를 사전정보로 주면 어떤 결과가 있는지\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "12.9\n",
    "reward에 대한 욕심이 없다? 그냥 계속 벽에 박는다. 앞으로 나아가는 것에 대해서 안좋게 인식.\n",
    "\n",
    "Possible solutions:\n",
    "중간에 reward\n",
    "벽에 부딪치면 stronger punishment (개선 가능성이 적어보인다)\n",
    "==>\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "벽에 부딪치는 것에 대한 punishment 증가 시킴.\n",
    "randomness 0.2로 설정해서 하니까 잘찾는다.\n",
    "100번으로는 학습이 잘 안된다.\n",
    "==> 400번 하면 학습이 어느 정도 잘 된다.\n",
    "\n",
    "더 먼 경로를 사전정보로 주면 어떤 결과가 있는지\n",
    "\n",
    "아래에 길 하나 더 뚫어보기 (best, medium, worst path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 연구에 써볼만한 환경 연구해보기 (3개)\n",
    "# cliff-walking\n",
    "# kelly coinflip\n",
    "\n",
    "# pacman\n",
    "\n",
    "# maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
